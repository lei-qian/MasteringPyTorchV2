{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.2\n",
    "#!pip install torchvision==0.17\n",
    "#!pip install matplotlib==3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.302056\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.851937\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.082965\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.895059\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.727018\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.409475\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.414661\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.650797\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.412143\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.351428\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.527398\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.315825\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.425685\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.360831\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.303605\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.598366\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.338775\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.474756\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.133264\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.206837\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.370594\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.058567\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.286371\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.040540\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.250551\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.269153\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.408359\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.194539\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.145749\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.315688\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.143750\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.365967\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.146696\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.367333\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.168351\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.129350\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.365911\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.180460\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.318562\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.065432\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.186988\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.027003\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.047308\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.022366\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.035353\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.214592\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.183119\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.298756\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.111804\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.171392\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.107488\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.072999\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.022177\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.258480\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.028906\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.189763\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.096042\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.033066\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.070427\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.171062\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.118683\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.086369\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.056956\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.099313\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.094247\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.264742\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.313465\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.132270\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.147293\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.264373\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.272803\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.306220\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.010011\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.362774\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.211228\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.066547\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.023163\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.430748\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.017803\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.047505\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.495928\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.362507\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.110360\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.132901\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.160681\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.047914\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.034285\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.202772\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.018640\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.371719\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.135406\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.188777\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.028411\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.104413\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.492725\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.035861\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.229403\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.162366\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.061200\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.007916\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.002447\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.447893\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.090092\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.012116\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.209180\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.106252\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.067988\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.037331\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.015666\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.270799\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.079270\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.086138\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.006307\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.062748\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.032670\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.223236\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.111641\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.219764\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.014513\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.078544\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.121747\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.277454\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.058647\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.232240\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.032269\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.078638\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.067520\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.365800\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.207274\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.275777\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.317974\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.011204\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.024344\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.063903\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.114045\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.090973\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.265148\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.093710\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.106734\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.048631\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.169253\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.078319\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.226531\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.009725\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.186978\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.139727\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.115868\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.195927\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.072820\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.143363\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.151095\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.180783\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.024771\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.092594\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.159690\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.043536\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.015002\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.049279\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.019920\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.050891\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.128301\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.003842\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.005587\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.035550\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.016209\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.018645\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.030065\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.024013\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.145572\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.021424\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.050914\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.026139\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.009229\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.078825\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.027521\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.098326\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.122146\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.040583\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.015531\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.105144\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.011852\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.059444\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.024078\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.025308\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.059965\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.014011\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.050519\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.031315\n",
      "\n",
      "Test dataset: Overall Loss: 0.0474, Overall Accuracy: 9843/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.007021\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.065864\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.012074\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.082090\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.238707\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.104224\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.017839\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.256177\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.016282\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.153804\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.015520\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.174947\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.141261\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.020193\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.005871\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.141996\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.136295\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.060169\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.005207\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.401842\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.027096\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.030013\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.017696\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.048680\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.140288\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.020664\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.013273\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.004766\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.077183\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.013173\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.354035\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.201517\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.042602\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.065724\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.025738\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.012014\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.048008\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.070962\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.007224\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.025788\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.033739\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.088303\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.098965\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.005829\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.218396\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.184265\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.071258\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.066525\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.182760\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.004238\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.048930\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.099524\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.173959\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.440084\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.053574\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.251350\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.005002\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.010916\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.012875\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.250165\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.096503\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.006533\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.097712\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.043563\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.068745\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.102417\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.384951\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.125028\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.038480\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.019430\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.203273\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.008021\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.018248\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.072491\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.006324\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.007714\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.023039\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.022515\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.038694\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.254940\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.005623\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.101290\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.207517\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.246722\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.020937\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.103291\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.048553\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.058269\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.063699\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.004765\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.014981\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.002575\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.135550\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.006865\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.071510\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.005777\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.095738\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.123302\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.027997\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.002557\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.008028\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.099586\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.070100\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.000958\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.027195\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.004513\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.043112\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.011463\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.005401\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.013835\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.210235\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.012887\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.001680\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.023628\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.044799\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.022366\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.073784\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.003746\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.031619\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.242865\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.357881\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.064858\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.007026\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.024751\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.002825\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.019074\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.054980\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.099709\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.001770\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.024091\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.037667\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.009987\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.107596\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.005566\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.001110\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.015416\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.027256\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.044479\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.003084\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.002633\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.011323\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.025614\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.293445\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.020505\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.004617\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.093285\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.020390\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.155366\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.031731\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.010554\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.019893\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.011126\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.102080\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.023148\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.010964\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.011264\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.001871\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.025313\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.018773\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.021403\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.031867\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.016637\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.038681\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.191821\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.004353\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.094356\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.011105\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.172378\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.004634\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.070904\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.020585\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.003359\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.004745\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.041436\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.008010\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.005893\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.004426\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.013298\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.010539\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.027467\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.006035\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.126322\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.010320\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.072157\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.111248\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.089788\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.010975\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.186341\n",
      "\n",
      "Test dataset: Overall Loss: 0.0404, Overall Accuracy: 9858/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "for epoch in range(1, 3):\n",
    "    #train(model, device, train_dataloader, optimizer, epoch)\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel prediction is : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround truth is : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_targets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcn2(x)\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leqian\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
